import { Injectable, HttpException, HttpStatus, Logger } from '@nestjs/common';
import { ConfigService } from '@nestjs/config';
import OpenAI from 'openai';
import { MessageDto, GptResponseDto } from './dto/message.dto';

@Injectable()
export class GptService {
  private readonly logger = new Logger(GptService.name);
  private openai: OpenAI;

  constructor(private configService: ConfigService) {
    this.logger.log('ğŸš€ Inicializando servicio ChatGPT...');
    
    const apiKey = this.configService.get<string>('OPENAI_API_KEY');
    
    // Log de verificaciÃ³n de API Key (sin mostrar la clave completa)
    if (apiKey) {
      this.logger.log(`ğŸ”‘ API Key encontrada: ${apiKey.substring(0, 10)}...${apiKey.substring(apiKey.length - 4)}`);
    } else {
      this.logger.error('âŒ API Key de OpenAI no encontrada en variables de entorno');
    }

    this.openai = new OpenAI({
      apiKey: apiKey,
    });
    
    this.logger.log('âœ… Cliente OpenAI inicializado correctamente');
  }

  async processMessage(messageDto: MessageDto, userEmail: string): Promise<GptResponseDto> {
    this.logger.log('ğŸ“¨ === INICIANDO PROCESAMIENTO DE MENSAJE ===');
    this.logger.log(`ğŸ‘¤ Usuario: ${userEmail}`);
    this.logger.log(`ğŸ’¬ Mensaje: "${messageDto.text}"`);
    this.logger.log(`ğŸ“ Longitud del mensaje: ${messageDto.text.length} caracteres`);

    const startTime = Date.now();

    try {
      this.logger.log('ğŸ¤– Enviando request a OpenAI...');
      this.logger.log('âš™ï¸ ParÃ¡metros del request:', {
        model: 'gpt-3.5-turbo',
        max_tokens: 500,
        temperature: 0.7,
        messages_count: 2
      });

      const completion = await this.openai.chat.completions.create({
        model: 'gpt-3.5-turbo',
        messages: [
          {
            role: 'system',
            content: `Eres un asistente Ãºtil y amigable en un chat en tiempo real. 
                     Responde de manera concisa y natural. 
                     El usuario que te escribiÃ³ es: ${userEmail}`
          },
          {
            role: 'user',
            content: messageDto.text
          }
        ],
        max_tokens: 500,
        temperature: 0.7,
      });

      const processingTime = Date.now() - startTime;
      this.logger.log(`â±ï¸ Tiempo de procesamiento: ${processingTime}ms`);

      // Log detallado de la respuesta
      this.logger.log('ğŸ“¥ Respuesta recibida de OpenAI:', {
        id: completion.id,
        model: completion.model,
        choices_length: completion.choices.length,
        usage: completion.usage,
        finish_reason: completion.choices[0]?.finish_reason
      });

      const response = completion.choices[0]?.message?.content;
      
      if (!response) {
        this.logger.error('âŒ OpenAI no devolviÃ³ contenido en la respuesta');
        this.logger.error('âŒ Completion object:', JSON.stringify(completion, null, 2));
        throw new HttpException(
          'No se pudo generar una respuesta',
          HttpStatus.INTERNAL_SERVER_ERROR
        );
      }

      this.logger.log(`âœ… Respuesta generada: "${response.substring(0, 100)}${response.length > 100 ? '...' : ''}"`);
      this.logger.log(`ğŸ“Š Tokens utilizados:`, {
        prompt: completion.usage?.prompt_tokens,
        completion: completion.usage?.completion_tokens,
        total: completion.usage?.total_tokens
      });

      const result: GptResponseDto = {
        text: response,
        timestamp: new Date(),
        usage: {
          prompt_tokens: completion.usage?.prompt_tokens || 0,
          completion_tokens: completion.usage?.completion_tokens || 0,
          total_tokens: completion.usage?.total_tokens || 0,
        }
      };

      this.logger.log('ğŸ‰ === PROCESAMIENTO EXITOSO ===');
      return result;

    } catch (error) {
      const processingTime = Date.now() - startTime;
      this.logger.error(`ğŸ’¥ === ERROR EN PROCESAMIENTO (${processingTime}ms) ===`);
      this.logger.error('âŒ Error completo:', error);
      
      // Log especÃ­fico segÃºn el tipo de error
      if (error.response) {
        this.logger.error('ğŸ” Error de respuesta HTTP:', {
          status: error.response.status,
          statusText: error.response.statusText,
          data: error.response.data
        });
      }

      if (error.code) {
        this.logger.error('ğŸ” CÃ³digo de error:', error.code);
      }

      if (error.message) {
        this.logger.error('ğŸ” Mensaje de error:', error.message);
      }

      // Manejo especÃ­fico de errores de OpenAI
      if (error.response?.status === 429) {
        this.logger.error('âš ï¸ Rate limit excedido');
        throw new HttpException(
          'LÃ­mite de requests excedido. Intenta de nuevo en unos minutos.',
          HttpStatus.TOO_MANY_REQUESTS
        );
      }

      if (error.response?.status === 401) {
        this.logger.error('ğŸ” Error de autenticaciÃ³n con OpenAI');
        throw new HttpException(
          'Error de autenticaciÃ³n con OpenAI',
          HttpStatus.INTERNAL_SERVER_ERROR
        );
      }

      if (error.response?.status === 400) {
        this.logger.error('ğŸ“ Error en la peticiÃ³n (Bad Request)');
        this.logger.error('ğŸ“ Mensaje enviado:', messageDto.text);
      }

      // Respuesta de fallback con logs
      this.logger.log('ğŸ”„ Generando respuesta de fallback...');
      const fallbackResponse: GptResponseDto = {
        text: `Lo siento, no pude procesar tu mensaje en este momento. Error: ${error.message}`,
        timestamp: new Date(),
      };

      this.logger.log('ğŸ“¤ Respuesta de fallback generada');
      return fallbackResponse;
    }
  }

  // MÃ©todo adicional para verificar la configuraciÃ³n
  async healthCheck(): Promise<{ status: string; openai: boolean; timestamp: Date }> {
    this.logger.log('ğŸ¥ Verificando estado del servicio OpenAI...');
    
    try {
      // Hacer una peticiÃ³n simple para verificar conectividad
      const testCompletion = await this.openai.chat.completions.create({
        model: 'gpt-3.5-turbo',
        messages: [{ role: 'user', content: 'Hello' }],
        max_tokens: 5,
      });

      this.logger.log('âœ… Health check exitoso con OpenAI');
      return {
        status: 'ok',
        openai: true,
        timestamp: new Date(),
      };
    } catch (error) {
      this.logger.error('âŒ Health check fallÃ³:', error.message);
      return {
        status: 'degraded',
        openai: false,
        timestamp: new Date(),
      };
    }
  }
}