import { Injectable, HttpException, HttpStatus, Logger } from '@nestjs/common';
import { ConfigService } from '@nestjs/config';
import OpenAI from 'openai';
import { MessageDto, GptResponseDto } from './dto/message.dto';

@Injectable()
export class GptService {
  private readonly logger = new Logger(GptService.name);
  private openai: OpenAI;

  constructor(private configService: ConfigService) {
    this.logger.log('üöÄ Inicializando servicio ChatGPT...');
    
    const apiKey = this.configService.get<string>('OPENAI_API_KEY');
    
    // Log de verificaci√≥n de API Key (sin mostrar la clave completa)
    if (apiKey) {
      this.logger.log(`üîë API Key encontrada: ${apiKey.substring(0, 10)}...${apiKey.substring(apiKey.length - 4)}`);
    } else {
      this.logger.error('‚ùå API Key de OpenAI no encontrada en variables de entorno');
    }

    this.openai = new OpenAI({
      apiKey: apiKey,
    });
    
    this.logger.log('‚úÖ Cliente OpenAI inicializado correctamente');
  }

  async processMessage(messageDto: MessageDto, userEmail: string): Promise<GptResponseDto> {
    this.logger.log('üì® === INICIANDO PROCESAMIENTO DE MENSAJE ===');
    this.logger.log(`üë§ Usuario: ${userEmail}`);
    this.logger.log(`üí¨ Mensaje: "${messageDto.text}"`);
    this.logger.log(`üìè Longitud del mensaje: ${messageDto.text.length} caracteres`);

    const startTime = Date.now();

    try {
      this.logger.log('ü§ñ Enviando request a OpenAI...');
      this.logger.log('‚öôÔ∏è Par√°metros del request:', {
        model: 'gpt-3.5-turbo',
        max_tokens: 500,
        temperature: 0.7,
        messages_count: 2
      });

      const completion = await this.openai.chat.completions.create({
        model: 'gpt-3.5-turbo',
        messages: [
          {
            role: 'system',
            content: `Eres un asistente √∫til y amigable en un chat en tiempo real. 
                     Responde de manera concisa y natural. 
                     El usuario que te escribi√≥ es: ${userEmail}`
          },
          {
            role: 'user',
            content: messageDto.text
          }
        ],
        max_tokens: 500,
        temperature: 0.7,
      });

      const processingTime = Date.now() - startTime;
      this.logger.log(`‚è±Ô∏è Tiempo de procesamiento: ${processingTime}ms`);

      // Log detallado de la respuesta
      this.logger.log('üì• Respuesta recibida de OpenAI:', {
        id: completion.id,
        model: completion.model,
        choices_length: completion.choices.length,
        usage: completion.usage,
        finish_reason: completion.choices[0]?.finish_reason
      });

      const response = completion.choices[0]?.message?.content;
      
      if (!response) {
        this.logger.error('‚ùå OpenAI no devolvi√≥ contenido en la respuesta');
        this.logger.error('‚ùå Completion object:', JSON.stringify(completion, null, 2));
        throw new HttpException(
          'No se pudo generar una respuesta',
          HttpStatus.INTERNAL_SERVER_ERROR
        );
      }

      this.logger.log(`‚úÖ Respuesta generada: "${response.substring(0, 100)}${response.length > 100 ? '...' : ''}"`);
      this.logger.log(`üìä Tokens utilizados:`, {
        prompt: completion.usage?.prompt_tokens,
        completion: completion.usage?.completion_tokens,
        total: completion.usage?.total_tokens
      });

      const result: GptResponseDto = {
        text: response,
        timestamp: new Date(),
        usage: {
          prompt_tokens: completion.usage?.prompt_tokens || 0,
          completion_tokens: completion.usage?.completion_tokens || 0,
          total_tokens: completion.usage?.total_tokens || 0,
        }
      };

      this.logger.log('üéâ === PROCESAMIENTO EXITOSO ===');
      return result;

    } catch (error) {
      const processingTime = Date.now() - startTime;
      this.logger.error(`üí• === ERROR EN PROCESAMIENTO (${processingTime}ms) ===`);
      this.logger.error('‚ùå Error completo:', error);
      
      // Log espec√≠fico seg√∫n el tipo de error
      if (error.response) {
        this.logger.error('üîç Error de respuesta HTTP:', {
          status: error.response.status,
          statusText: error.response.statusText,
          data: error.response.data
        });
      }

      if (error.code) {
        this.logger.error('üîç C√≥digo de error:', error.code);
      }

      if (error.message) {
        this.logger.error('üîç Mensaje de error:', error.message);
      }

      // Manejo espec√≠fico de errores de OpenAI
      if (error.response?.status === 429) {
        this.logger.error('‚ö†Ô∏è Rate limit excedido');
        throw new HttpException(
          'L√≠mite de requests excedido. Intenta de nuevo en unos minutos.',
          HttpStatus.TOO_MANY_REQUESTS
        );
      }

      if (error.response?.status === 401) {
        this.logger.error('üîê Error de autenticaci√≥n con OpenAI');
        throw new HttpException(
          'Error de autenticaci√≥n con OpenAI',
          HttpStatus.INTERNAL_SERVER_ERROR
        );
      }

      if (error.response?.status === 400) {
        this.logger.error('üìù Error en la petici√≥n (Bad Request)');
        this.logger.error('üìù Mensaje enviado:', messageDto.text);
      }

      // Respuesta de fallback con logs
      this.logger.log('üîÑ Generando respuesta de fallback...');
      const fallbackResponse: GptResponseDto = {
        text: `Lo siento, no pude procesar tu mensaje en este momento. Error: ${error.message}`,
        timestamp: new Date(),
      };

      this.logger.log('üì§ Respuesta de fallback generada');
      return fallbackResponse;
    }
  }

  // M√©todo adicional para verificar la configuraci√≥n
  async healthCheck(): Promise<{ status: string; openai: boolean; timestamp: Date }> {
    this.logger.log('üè• Verificando estado del servicio OpenAI...');
    
    try {
      // Hacer una petici√≥n simple para verificar conectividad
      const testCompletion = await this.openai.chat.completions.create({
        model: 'gpt-3.5-turbo',
        messages: [{ role: 'user', content: 'Hello' }],
        max_tokens: 5,
      });

      if (!testCompletion || !testCompletion.choices || testCompletion.choices.length === 0) {
        throw new Error('No se recibi√≥ una respuesta v√°lida de OpenAI');
      }
      this.logger.log('‚úÖ Health check exitoso con OpenAI');
      return {
        status: 'ok',
        openai: true,
        timestamp: new Date(),
      };
    } catch (error) {
      this.logger.error('‚ùå Health check fall√≥:', error.message);
      return {
        status: 'degraded',
        openai: false,
        timestamp: new Date(),
      };
    }
  }
}